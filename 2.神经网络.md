## 神经网络

上一章我们学习了感知机。关于感知机，既有好消息，也有坏消息。好 消息是，即便对于复杂的函数，感知机也隐含着能够表示它的可能性。上一 章已经介绍过，即便是计算机进行的复杂处理，感知机(理论上)也可以将 其表示出来。**坏消息是，设定权重的工作，即确定合适的、能符合预期的输入与输出的权重，现在还是由人工进行的**。上一章中，我们结合与门、或门的真值表人工决定了合适的权重。

神经网络的出现就是为了解决刚才的坏消息。具体地讲，**神经网络的一个重要性质是它可以自动地从数据中学习到合适的权重参数**。本章中，我们 会先介绍神经网络的概要，然后重点关注神经网络进行识别时的处理。在下 一章中，我们将了解如何从数据中学习权重参数。

### 激活函数

#### 什么是激活函数？
激活函数就像是神经网络中的"开关"，它决定神经元是否要"激活"（输出信号）。想象一下，当你碰到很烫的东西时，你的神经会立即做出反应，让你缩手 - 激活函数就是类似这样的机制。

#### 为什么需要激活函数？
1. **引入非线性**：没有激活函数，神经网络就只能处理线性关系。有了激活函数，网络就能学习更复杂的模式。
2. **控制信号**：激活函数帮助控制信号的强度，决定是否传递这个信号。

#### 常见的激活函数
1. **阶跃函数**：
   - 最简单的激活函数
   - 输入大于某个值时输出1，否则输出0
   - 就像普通的开关，要么开要么关

2. **Sigmoid函数**：
   - 想象一个温和的刹车系统：不是急刹车（突然从1变成0），而是缓慢地降速
   - 输出永远在0到1之间，就像一个百分比，告诉你事情发生的可能性
   - 形状像一个被压扁的S，所以叫Sigmoid（S形的）
   - 特别适合表达"概率"，比如预测一个邮件是不是垃圾邮件的可能性

3. **ReLU函数**（修正线性单元）：
   - 现在最常用的激活函数
   - 输入大于0时，输出就是输入值
   - 输入小于0时，输出0
   - 就像一个"只允许正数通过"的过滤器

#### 举个例子 
想象你在调整空调温度：
- 阶跃函数就像普通空调：到了设定温度就完全关闭
- Sigmoid函数就像变频空调：可以逐渐调节制冷强度
- ReLU函数就像单向阀门：只让热量单向流动   


#### Sigmoid函数详解

Sigmoid函数的公式是：$h(x) = \frac{1}{1 + e^{-x}}$

让我们通过简单的方式来理解它：

1. **基本特征**：
   - 不管输入是什么数字，输出都会被"压缩"到0和1之间
   - 输入越大，输出越接近1；输入越小，输出越接近0
   - 在中间区域变化平缓，两端变化缓慢

2. **生活中的例子**：
   - 就像调节音量的推子：
     - 在中间位置时，稍微推动就能听出音量变化
     - 但在最大或最小音量附近，推动很大幅度可能只有很小的变化

3. **实际应用**：
   - 特别适合表示"可能性"，比如：
     - 一个学生及格的可能性
     - 一张图片是猫的可能性
     - 今天会下雨的可能性

4. **为什么叫"Sigmoid"**：
   - 因为它的图像像一个拉长的"S"形状
   - "Sigmoid"就是"S形的"意思

##### 具体的计算过程
让我用更简单的方式解释这个公式：$h(x) = \frac{1}{1 + e^{-x}}$

想象一下这个过程：

1. **输入部分**：
   - x 是我们输入的任何数字
   - e 是一个固定的数（约2.71828）
   - e^(-x) 就是e的-x次方

2. **举个具体例子**：
   - 如果输入x = 0：
     - e^0 = 1
     - 所以h(0) = 1/(1+1) = 0.5
   
   - 如果输入x = 2：
     - e^(-2) 很小（约0.135）
     - 所以h(2) ≈ 1/(1+0.135) ≈ 0.88（接近1）
   
   - 如果输入x = -2：
     - e^2 较大（约7.389）
     - 所以h(-2) ≈ 1/(1+7.389) ≈ 0.12（接近0）

3. **生活中的比喻**：
   - 就像把一个数字通过"挤压"，永远得到0到1之间的结果
   - 类似于把任意长度的绳子都压缩到1米以内，但永远不会完全到达0米或1米

4. **实际意义**：
   - 这个函数可以把任何数字转换成"概率"
   - 比如：及格概率、下雨概率等

所以，不管输入什么数字，这个函数都会给出一个0到1之间的数，这就是为什么它在表示概率时特别有用。   


让我用一个生活中的例子来帮你理解Sigmoid函数：

想象你在开车，前面有一个红绿灯：

1. **基本工作原理**：
   - 你的车速（输入x）决定了你停车的可能性（输出值，在0到1之间）
   - 当你离红绿灯很远时（x是很大的负数）：
     - 停车的可能性接近0（继续开）
   
   - 当你正好在临界距离时（x接近0）：
     - 停车的可能性在0.5左右（犹豫要不要停）
     
   - 当你已经很接近红绿灯时（x是很大的正数）：
     - 停车的可能性接近1（一定要停）

2. **为什么是平滑的曲线**：
   - 不像普通的开关（非0即1）
   - 而是像汽车的刹车，是一个渐变的过程
   - 你不会突然从100km/h直接变成0km/h
   - 而是会慢慢减速，直到完全停下

3. **实际应用举例**：
   - 预测下雨概率：
     - 温度、湿度等天气数据作为输入x
     - Sigmoid函数输出一个0到1之间的数，表示下雨的可能性
     - 比如0.8表示80%的概率会下雨

这样理解的话，Sigmoid函数就是一个把任何数字都转换成"可能性"的工具，而且这个转换是平滑的，不会有突然的跳变。


#### ReLU函数详解
ReLU函数的公式很简单：$f(x) = max(0, x)$

1. **基本特征**：
   - 计算简单：只需要判断是否大于0
   - 梯度传播效率高：正区间梯度始终为1
   - 解决了传统Sigmoid函数的梯度消失问题
   - 产生了真正的稀疏性：负值直接变为0

2. **优点**：
   - 计算速度快：比Sigmoid和tanh快很多
   - 收敛速度快：在正区间梯度不会消失
   - 稀疏激活：可以让网络变得稀疏
   - 生物学合理性：更符合生物神经元的特性

3. **缺点**：
   - 神经元死亡问题：如果一个神经元进入负区间，它的梯度将永远为0
   - 输出不是零中心化的：所有输出都是正数
   
4. **变体**：
   - Leaky ReLU：给负区间一个很小的斜率，避免神经元死亡
   - PReLU：可学习的Leaky ReLU，斜率是一个可训练的参数
   - ELU：负区间使用指数函数，使输出更接近零中心

5. **实际应用场景**：
   - 深度卷积神经网络
   - 计算机视觉任务
   - 语音识别系统
   - 自然语言处理

6. **为什么现在最常用？**
   - 简单高效：计算和实现都很简单
   - 性能优秀：在大多数情况下效果都很好
   - 训练稳定：不容易出现梯度消失
   - 计算资源友好：计算量小，训练快

#### ReLU函数是什么？
想象你有一个特殊的过滤器：
- 当输入是正数时，它让数字原封不动地通过
- 当输入是负数时，它直接输出0

比如：
- 输入5 → 输出5
- 输入3 → 输出3
- 输入0 → 输出0
- 输入-2 → 输出0
- 输入-5 → 输出0

#### 生活中的例子

1. **像一个单向门**
   - 就像商场的单向旋转门
   - 只能从一个方向推开（正数可以通过）
   - 另一个方向完全推不动（负数变成0）

2. **像一个防晒霜**
   - 阳光（正数）可以适度透过
   - 有害光线（负数）被完全阻挡

3. **像一个游戏积分规则**
   - 赢了（正数）就得到相应分数
   - 输了（负数）分数不会低于0

#### 为什么叫"修正线性单元"？

- "修正"：因为它修正了负数，把所有负数都改成0
- "线性"：在正数部分，它保持线性关系（直线）
- "单元"：因为它是神经网络中的一个基本单位

这就是为什么它被称为ReLU（Rectified Linear Unit，修正线性单元）。它的简单性和有效性使它成为现代深度学习中最受欢迎的激活函数。

> 让我解释一下什么是"梯度消失问题"以及ReLU如何解决它。

让我解释一下什么是"梯度消失问题"以及ReLU如何解决它。

#### 什么是梯度消失问题？

想象你在爬一座山：
1. **Sigmoid函数的情况**：
   - 在山顶和山脚（函数两端）时，地面几乎是平的
   - 即使你走很远的距离，高度也几乎不变
   - 这就像梯度（坡度）几乎消失了
   - 在神经网络中，这意味着参数几乎不会更新

2. **为什么这是个问题？**
   - 就像在平地上找最高点一样困难
   - 网络很难学习，因为参数更新太小
   - 特别是在深层网络中，这个问题会变得更严重

#### ReLU如何解决这个问题？

1. **ReLU的特点**：
   - 在正数区间：斜率始终是1，保持恒定
   - 在负数区间：直接输出0
   
2. **为什么这样能解决问题？**
   - 就像在爬山时：
     - 只要在正数区域，坡度始终保持不变
     - 不会出现"太平缓以至于分不清方向"的情况
     - 梯度（坡度）始终是1，信号可以有效传递

#### 生活中的比喻

想象你在调节音响音量：
- **Sigmoid**：在最大或最小音量附近，即使你大幅度转动旋钮，音量变化也很小
- **ReLU**：只要在正数区域，转动旋钮多少，音量就变化多少，关系始终是线性的

这就是为什么ReLU能够解决梯度消失问题 —— 它在正数区间保持了稳定的梯度传递，使得神经网络的训练变得更加容易和高效。

### 矩阵(二维数组)

让我重新从最基础解释矩阵。

#### 矩阵的基本概念

1. **什么是矩阵？**
   - 矩阵就是按照行和列排列的数字表格
   - 就像Excel表格一样，有行有列
   ```
   [1  2  3]
   [4  5  6]
   ```
   这就是一个2行3列的矩阵

2. **生活中的例子**
   
   想象一个班级的成绩单：
   ```
          语文  数学  英语
   小明    85    92    78
   小红    90    88    85
   小华    76    95    89
   ```
   这就是一个3×3的矩阵

#### 矩阵的基本操作

1. **如何看矩阵的大小**
   - 数一数有几行几列就行
   - 比如上面的成绩单是3行3列，记作3×3矩阵

2. **如何找到某个具体的数**
   - 先找行，再找列
   - 比如要找小红的数学成绩：
     - 先找第2行（小红）
     - 再找第2列（数学）
     - 得到88分

#### 为什么需要矩阵？

1. **整理数据**
   - 就像超市用表格整理商品信息
   - 学校用表格记录学生成绩
   - 医院用表格记录病人信息

2. **批量计算**
   - 比如要算每个学生的平均分
   - 或者要给所有成绩加5分

#### 小结

矩阵就是一种整理数据的方式，就像我们平时用的表格一样。它的优点是：
- 数据排列整齐，容易查找
- 可以进行批量计算
- 适合计算机处理

> 详细介绍下矩阵为何在神经网络中如此重要？

让我详细解释矩阵在神经网络中的重要性。

#### 1. 数据表示和存储

1. **输入数据的表示**
   ```
   [图片1的像素值]  [0.2  0.5  0.8  ...]
   [图片2的像素值]  [0.1  0.3  0.7  ...]
   [图片3的像素值]  [0.4  0.6  0.9  ...]
   ```
   - 一张图片可以表示为一行数字
   - 多张图片组成一个矩阵
   - 方便批量处理多个输入

2. **权重的存储**
   ```
   [w11  w12  w13]  // 第1个神经元的权重
   [w21  w22  w23]  // 第2个神经元的权重
   [w31  w32  w33]  // 第3个神经元的权重
   ```
   - 每行代表一个神经元的所有连接权重
   - 整齐有序，便于更新和管理

#### 2. 并行计算优势

1. **批量处理**
   - 一次可以处理多个样本
   - 比如同时处理100张图片
   - 大大提高训练效率

2. **硬件加速**
   - GPU专门优化了矩阵运算
   - 可以同时进行大量并行计算
   - 训练速度提升几十甚至上百倍

#### 3. 实际应用举例

想象一个图像识别任务：
1. **没有矩阵**：
   ```
   for 每张图片:
       for 每个像素:
           for 每个权重:
               逐个计算
   ```

2. **使用矩阵**：
   ```
   输入矩阵 × 权重矩阵 = 一次性得到所有结果
   ```

#### 4. 为什么更高效？

1. **代码简洁**
   - 复杂的神经网络运算可以用简单的矩阵运算表示
   - 减少循环，提高代码可读性

2. **计算效率**
   - 一次矩阵运算替代多重循环
   - 充分利用现代硬件特性
   - 显著提升训练速度

这就是为什么矩阵成为神经网络的基础数据结构 —— 它不仅让数据管理变得简单，还能极大提升计算效率。

### 矩阵乘法

![这是图片](./images/矩阵乘法.png)

让我根据图片解释矩阵乘法。

#### 1. 矩阵的表示方式（图3-10）

首先要理解矩阵的结构：
- 横向排列的数字称为"行"
- 纵向排列的数字称为"列"
- 图中的矩阵是3×2的矩阵（3行2列）
```
[1  2]
[3  4]
[5  6]
```

#### 2. 矩阵乘法的计算（图3-11）

以图中的例子来说：
```
矩阵A:          矩阵B:
[1  2]         [5  6]
[3  4]    ×    [7  8]
```

计算第一个结果19的过程：
1. 取A的第1行：[1  2]
2. 取B的第1列：[5] [7]
3. 相乘相加：1×5 + 2×7 = 19

计算第二个结果43的过程：
1. 取A的第2行：[3  4]
2. 取B的第1列：[5] [7]
3. 相乘相加：3×5 + 4×7 = 43


计算第三个结果22的过程（第一行第二列的结果）：
1. 取第一个矩阵的第1行：[1  2]
2. 取第二个矩阵的第2列：[6] [8]
3. 相乘相加：1×6 + 2×8 = 6 + 16 = 22

计算第四个结果50的过程 要得到50（第二行第二列的结果）：

1. 取第一个矩阵的第2行：[3  4]
2. 取第二个矩阵的第2列：[6] [8]
3. 相乘相加：3×6 + 4×8 = 18 + 32 = 50

#### 3. 图中的箭头含义

图中的箭头表示了计算路径：
- 1×5 + 2×7 = 19  （第一行第一列的结果）
- 3×5 + 4×7 = 43  （第二行第一列的结果）
- 1×6 + 2×8 = 22   (第一行第二列的结果)
- 3×6 + 4×8 = 50   (第二行第二列的结果)

#### 4. 完整的结果矩阵
```
[19  22]
[43  50]
```

每个位置的数字都是通过这样的"行×列+行×列"得到的。这就是矩阵乘法的基本计算方式。


#### 注意

![这是图片](./images/3-2.png)

在矩阵乘法中，“对应维度的元素个数保持一致”是确保矩阵能够相乘的关键规则。我们通过一个例子详细说明其含义和应用：

##### 基本规则

若矩阵 A 的形状为 $m \times n$，矩阵 B 的形状为 $p \times q$，则 A 能与 B 相乘当且仅当：
$$n = p$$
即 **A 的列数（第2个维度）**必须等于 B 的行数（第1个维度）。结果矩阵 C 的形状为 $m \times q$。
以下用具体案例解析：

示例：形状为 $3 \times 2$ 的 A 矩阵与 $2 \times 4$ 的 B 矩阵相乘

##### 维度匹配分析

A 的列数（2）等于 B 的行数（2），满足乘法条件。结果 **C $3 \times 4$。

计算过程
每个元素 $c_{ij}$ 是通过 A 的第 i 行与 B 的第 j 列的点积（对应元素相乘再求和）得到的。例如：
$$c_{11} = a_{11} \cdot b_{11} + a_{12} \cdot b_{21}$$

关键：只有当 A 的列数（即每行元素个数）等于 B 的行数（即每列元素个数）时，点积运算才能对齐并完成计算。

##### 为什么要求“对应维度一致”？

###### 数学本质
矩阵乘法本质上是 线性组合的叠加。每一行代表一个输出向量，每列代表一个输入方向。只有在输入方向上的元素数量一致时，组合才具有意义。

直观解释：如果 A 有 2 列，B 必须有 2 行，相当于每个输入分量（A 的列）需要对应一个权重（B 的行）来生成输出。
物理意义（神经网络中的应用）

###### 输入层与权重层的匹配：
如果将输入向量 $\mathbf{x}$（形状：$2 \times 1$）与权重矩阵 $W$（形状：$2 \times 4$）相乘，结果会是 $4 \times 1$，对应下一层的 4 个神经元。如果 $W$ 的形状为 $3 \times 4$，则维度不匹配（因输入无法对齐 W 的列），导致无法计算。

###### 反向传播的梯度更新：
在神经网络训练时，梯度计算同样依赖维度一致性。例如，$\frac{\partial L}{\partial W}$ 的维度必须与 $W$ 的原始维度相同，否则梯度下降更新参数时会出错（如权重矩阵形状突然改变）。


###### 常见错误及解决方法

错误类型
试图将 $3 \times 2$ 的矩阵 A 和 $3 \times 4$ 的矩阵 B 直接相乘时，会因 $2 \neq 3$ 而报错。
调整方法

转置矩阵：若需相乘，需要将 B 转置为 $4 \times 3$，此时 A ($3 \times 2$) × B$^T$ ($4 \times 3$) 的维度为 $3 \times 4 \times 3 \times 3$，仍不匹配；因此并非所有情况都能通过转置解决，需重新设计维度。重新设计网络结构：神经网络的层级设计中需规划每层的神经元数目，使权重矩阵维度匹配（如设置第 N 层的输出维度等于第 N+1 层的输入维度）。


###### 总结
“对应维度元素个数保持一致”的本质是确保线性代数中向量空间的映射关系合法。其核心意义如下：

数学层面：点积运算要求分量数量对齐。应用层面：在神经网络、图像处理等领域，错误的维度会导致模型无法前向传播或反向更新，从而彻底失效。
通过静态检查矩阵形状，并理解维度对齐的原理，可以避免大多数与矩阵运算相关的错误。


### 神经网络

![这是图片](./images/3-15.png)

#### 各层间信号传递实现

![这是图片](./images/3-17.png)
![这是图片](./images/3-18.png)
![这是图片](./images/3-19.png)
![这是图片](./images/3-20.png)

> 推断过程

让我用手写数字"7"的例子来解释神经网络的推断过程：

##### 1. 输入层（有784个神经元）
```
输入：28×28=784个像素点
[0.0, 0.0, 0.9, 0.9, ..., 0.0]  // 784个数字
```
- 0.0表示白色背景
- 0.9表示数字笔画
- 每个像素都被归一化到0-1之间

##### 2. 第一隐藏层
```
A¹ = X·W¹ + B¹
```
- 每个神经元关注不同的特征
- 神经元1可能检测横线
- 神经元2可能检测竖线
- 神经元3可能检测斜线
- 这些是"7"的基本组成部分

##### 3. 第二隐藏层
```
A² = sigmoid(A¹)·W² + B²
```
- 组合第一层学到的基本特征
- 某个神经元可能专门识别"7"的上横
- 另一个可能识别"7"的斜杠
- 形成更复杂的特征组合

##### 4. 输出层（10个神经元）
```
Y = softmax(A²·W³ + B³)
[0.01, 0.02, 0.01, 0.03, 0.02, 0.01, 0.85, 0.02, 0.02, 0.01]
```
- 每个神经元对应一个数字（0-9）
- 对于"7"的图片：
  - 第7个神经元输出最高概率（85%）
  - 其他数字的概率很低
  - 网络认为这是数字"7"的可能性最大

##### 生活中的比喻
就像老师判断学生写的数字：
1. 看到笔画（输入层）
2. 发现有横线和斜线（第一隐藏层）
3. 认出这些线组成了"7"的形状（第二隐藏层）
4. 最终确定这是数字"7"（输出层）

这就是神经网络如何一步步从像素点推断出这是数字"7"的过程。

### 输出层设计

#### 恒等函数

让我来介绍神经网络中的恒等函数（Identity Function）。

##### 1. 什么是恒等函数？

恒等函数的定义非常简单：
```
f(x) = x
```
也就是说，输入什么就输出什么，不做任何变换。

##### 2. 图形特征

- 在坐标系中是一条直线
- 斜率为1
- 通过原点(0,0)
- 输入和输出完全相等

##### 3. 为什么需要恒等函数？

1. **输出层的激活函数**
   - 在回归问题中常用
   - 当我们需要输出实数值时
   - 比如预测房价、温度等

2. **特征保持**
   - 保持原始数据的特征
   - 不对数据进行任何变换
   - 适合直接输出原始数值的场景

##### 4. 与其他激活函数的对比

1. **与Sigmoid的区别**：
   - Sigmoid：输出范围(0,1)
   - 恒等函数：输出范围(-∞,+∞)

2. **与ReLU的区别**：
   - ReLU：负数输出为0
   - 恒等函数：负数保持不变

##### 5. 使用场景

1. **回归问题**：
   - 预测房价
   - 预测气温
   - 预测股票价格

2. **特征提取**：
   - 当需要保持原始特征时
   - 不希望对数据做任何变换

##### 6. 生活中的比喻

想象一面镜子：
- 你的动作是输入
- 镜子里的影像是输出
- 完全忠实地反映输入

这就是恒等函数的本质：忠实地传递信息，不做任何改变。

#### softmax

分类问题中使用的 softmax 函数可以用下面的式(3.10)表示。


让我解释一下图中的Softmax函数公式：

$y_k = \frac{exp(a_k)}{\sum_{i=1}^n exp(a_i)}$

##### 1. 公式解析

- $y_k$：第k个输出的概率值
- $a_k$：第k个神经元的输入值
- $exp()$：自然指数函数（e的幂）
- $\sum_{i=1}^n$：对所有神经元的输出求和
- n：输出神经元的总数

##### 2. 计算步骤

1. 对每个输入值取指数：$exp(a_k)$
2. 计算所有指数之和：$\sum_{i=1}^n exp(a_i)$
3. 用第k个指数除以总和

##### 3. 特点

1. **归一化**：
   - 所有输出值的和等于1
   - 每个输出值都在0到1之间

2. **概率分布**：
   - 输出可以解释为概率
   - 适合分类问题

##### 4. 举例

假设有三个输入值：[2.0, 1.0, 0.1]

1. 计算指数：
   - $exp(2.0) = 7.389$
   - $exp(1.0) = 2.718$
   - $exp(0.1) = 1.105$

2. 求和：
   - 总和 = 7.389 + 2.718 + 1.105 = 11.212

3. 归一化：
   - $y_1 = 7.389/11.212 = 0.659$ (约66%)
   - $y_2 = 2.718/11.212 = 0.242$ (约24%)
   - $y_3 = 1.105/11.212 = 0.099$ (约10%)

这就是为什么Softmax函数常用于多分类问题的输出层，它能将神经网络的原始输出转换为概率分布。

让我用更简单的方式来解释Softmax函数。

##### 1. 生活中的例子：分配零花钱

想象妈妈要给三个孩子分配100元零花钱：
- 老大表现最好，得到60元
- 二娃表现一般，得到30元
- 小弟表现较差，得到10元

这就像Softmax函数：
- 输入：三个孩子的表现分数
- 输出：每个孩子分到的钱的比例（60%、30%、10%）
- 特点：所有输出加起来等于100%

##### 2. 计算步骤（用简单数字）

假设三个孩子的原始分数是：[4, 2, 1]

1. **第一步**：把分数变大（取指数）
   - 4 变成 约55
   - 2 变成 约7
   - 1 变成 约3
   
2. **第二步**：算总和
   - 55 + 7 + 3 = 65

3. **第三步**：每个数除以总和
   - 55/65 ≈ 0.85 (85%)
   - 7/65 ≈ 0.11 (11%)
   - 3/65 ≈ 0.04 (4%)

##### 3. 为什么要用它？

就像分配零花钱：
- 谁表现好，谁就得到更多
- 所有人加起来必须是100%
- 不会出现负数或超过100%的情况

##### 4. 实际应用

比如识别手写数字：
- 输入：一张数字图片
- 输出：是每个数字的可能性
  - "这是1的可能性是80%"
  - "这是7的可能性是15%"
  - "其他数字的可能性加起来是5%"

这样就能知道这个图片最可能是哪个数字了！

> 4是怎么变成55的？

让我解释一下为什么4会变成约55。这涉及到指数运算（e的幂）。

##### 简单计算过程

```
e ≈ 2.71828（自然对数的底数）

e^4 = 2.71828 × 2.71828 × 2.71828 × 2.71828
    ≈ 54.598 
    ≈ 55（四舍五入）
```

##### 生活中的例子

想象一个细菌繁殖的过程：
- 每小时细菌数量翻倍（约等于e倍）
- 第1小时：2.7个
- 第2小时：7.4个
- 第3小时：20.1个
- 第4小时：54.6个

这就是为什么4经过指数运算后会变成约55。指数增长是非常快的，这也是为什么在Softmax函数中，较大的输入值会主导最终的概率分布。

##### 实际应用中

在Softmax函数中，这种指数运算的特性很有用：
- 大的数字变得更大
- 小的数字相对变得更小
- 帮助突出显示最可能的选项

比如在图像识别中：
- 如果模型认为图片很像猫，给出分数4
- 经过指数运算后变成55
- 相比之下，其他动物的低分数经过指数运算后会变得更小
- 最终使得"是猫"的概率更大

#### 改进softmax函数

![这是图片](./images/3-11.png)

让我解释一下图片中展示的Softmax函数改进实现。

这个公式展示了一个数值计算上更稳定的Softmax实现方式。让我用简单的方式解释：

##### 1. 基本思路

原始公式：
```
y_k = exp(a_k) / Σexp(a_i)
```

改进公式：
```
y_k = (C·exp(a_k)) / (C·Σexp(a_i))
    = exp(a_k + logC) / Σexp(a_i + logC)
    = exp(a_k + C') / Σexp(a_i + C')
```

##### 2. 为什么要这样改？

想象一个具体例子：
- 原始输入：[1000, 990, 980]
- 直接计算exp(1000)会得到一个超大的数
- 可能导致计算机无法处理（数值溢出）

##### 3. 解决方案

1. **减去最大值**
   - 找到输入中的最大值（比如1000）
   - 所有数都减去这个最大值
   - 变成：[0, -10, -20]
   - 结果不变，但避免了数值太大

2. **实际例子**：
   ```python
   x = [1000, 990, 980]
   c = max(x)  # c = 1000
   y = [0, -10, -20]  # x - c
   
   # 现在可以安全计算exp
   exp_y = [1, 0.000045, 0.000000002]
   ```

##### 4. 为什么结果一样？

因为：
- 分子和分母同时除以同一个数
- 比例关系保持不变
- 最终的概率分布完全相同

这就像是：
- 60/100 = 0.6
- (60×2)/(100×2) = 0.6
- 虽然中间数字不同，但结果是一样的

这个改进让Softmax函数在处理大数字时更稳定，不容易出现数值计算问题。

##### python实现

```python
def softmax(a):
   c = np.max(a)
   exp_a = np.exp(a - c) # 溢出对策 sum_exp_a = np.sum(exp_a)
   y = exp_a / sum_exp_a
   return y
```